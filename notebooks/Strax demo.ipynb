{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strax analysis demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jelle, May 2018\n",
    "\n",
    "This notebook shows how to do basic high-level analysis with strax, much like `hax.minitrees`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, here are some jargon terms which we will introduce below:\n",
    "\n",
    "  * **Context**: Holds configuration on how to process\n",
    "  * **Dataframe** or **array**: table of related information produced by a plugin.\n",
    "  * **Plugin**: an algorithm that produces a dataframe\n",
    "  * **Data type**: specification of which columns are in a dataframe. \n",
    "  * **Data kind**: e.g. 'events' or 'peaks'. Dataframes of the same kind  have the same number of rows and can be merged.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we setup a strax **context**, much like `hax.init()`. A strax context contains all information on *how* to process: where to store what files, what plugins provide what data, etc. \n",
    "\n",
    "Unlike `hax.init`, you can have multiple active contexts, e.g. to load analysis and MC data, or compare data processed with different settings (we will see examples of this below).\n",
    "\n",
    "In the future, a contexts suitable for analysis should load from a config file (again like `hax.init`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import strax\n",
    "st = strax.Context(\n",
    "    storage=[strax.ZipFileStore('./processed'),\n",
    "             strax.SQLLiteStore()], \n",
    "    register_all=strax.xenon.plugins)\n",
    "\n",
    "run_id = '180423_1021'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# This just ensures some comments in dataframes below display nicely\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suposse we want to make an S1/S2 plot. We have to figure out which type of **dataframes** to load. A specific type of dataframe is also called a **data type**. (in hax these were called minitrees)\n",
    "\n",
    "Unlike hax, we can find this out automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1_index is part of event_basics (provided by EventBasics)\n",
      "s1_area is part of event_basics (provided by EventBasics)\n",
      "s1_area_fraction_top is part of event_basics (provided by EventBasics)\n",
      "s1_range_50p_area is part of event_basics (provided by EventBasics)\n",
      "s1_n_competing is part of event_basics (provided by EventBasics)\n"
     ]
    }
   ],
   "source": [
    "st.search_field('s1*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we're after a dataframe called event_basics. In the current context, this is provided by a **plugin** called EventBasics (but this doesn't concern us yet). Let's see what else is in event_basics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Field name</th>\n",
       "      <th>Data type</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n_peaks</td>\n",
       "      <td>int32</td>\n",
       "      <td>Number of peaks in the event</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drift_time</td>\n",
       "      <td>int64</td>\n",
       "      <td>Drift time between main S1 and S2 in ns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s1_index</td>\n",
       "      <td>int32</td>\n",
       "      <td>Main S1 peak index</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s1_area</td>\n",
       "      <td>float32</td>\n",
       "      <td>Main S1 area (PE)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s1_area_fraction_top</td>\n",
       "      <td>float32</td>\n",
       "      <td>Main S1 area fraction top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>s1_range_50p_area</td>\n",
       "      <td>float32</td>\n",
       "      <td>Main S1 width (ns, 50% area)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>s1_n_competing</td>\n",
       "      <td>int32</td>\n",
       "      <td>Main S1 number of competing peaks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>s2_index</td>\n",
       "      <td>int32</td>\n",
       "      <td>Main S2 peak index</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>s2_area</td>\n",
       "      <td>float32</td>\n",
       "      <td>Main S2 area (PE)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>s2_area_fraction_top</td>\n",
       "      <td>float32</td>\n",
       "      <td>Main S2 area fraction top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>s2_range_50p_area</td>\n",
       "      <td>float32</td>\n",
       "      <td>Main S2 width (ns, 50% area)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>s2_n_competing</td>\n",
       "      <td>int32</td>\n",
       "      <td>Main S2 number of competing peaks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Field name Data type                                  Comment\n",
       "0                n_peaks     int32             Number of peaks in the event\n",
       "1             drift_time     int64  Drift time between main S1 and S2 in ns\n",
       "2               s1_index     int32                       Main S1 peak index\n",
       "3                s1_area   float32                        Main S1 area (PE)\n",
       "4   s1_area_fraction_top   float32                Main S1 area fraction top\n",
       "5      s1_range_50p_area   float32             Main S1 width (ns, 50% area)\n",
       "6         s1_n_competing     int32        Main S1 number of competing peaks\n",
       "7               s2_index     int32                       Main S2 peak index\n",
       "8                s2_area   float32                        Main S2 area (PE)\n",
       "9   s2_area_fraction_top   float32                Main S2 area fraction top\n",
       "10     s2_range_50p_area   float32             Main S2 width (ns, 50% area)\n",
       "11        s2_n_competing     int32        Main S2 number of competing peaks"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.data_info('event_basics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like what we're after. Let's load it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = st.get_df(run_id, 'event_basics')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import petl as etl\n",
    "import sqlite3\n",
    "\n",
    "connection = sqlite3.connect('strax.db')\n",
    "\n",
    "def getTables(conn):\n",
    "   \"\"\"\n",
    "   Get a list of all tables\n",
    "   \"\"\"\n",
    "   cursor = conn.cursor()\n",
    "   cmd = \"SELECT name FROM sqlite_master WHERE type='table'\"\n",
    "   cursor.execute(cmd)\n",
    "   names = [row[0] for row in cursor.fetchall()]\n",
    "   return names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getTables(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psycopg2.extensions import register_adapter, AsIs\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "for typ in ['int8', 'int16', 'int32', 'int64',\n",
    "            'float16', 'float32', 'float64', 'float128']:\n",
    "    register_adapter(np.__getattribute__(typ), AsIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "connection = psycopg2.connect(dbname='dev',\n",
    "                    user='tunnell',\n",
    "                    host='devtest.ckalpnpalbhp.us-east-1.rds.amazonaws.com',\n",
    "                    password='W1MPfinder',\n",
    "                    port=5432)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table3 = etl.fromdataframe(df)\n",
    "etl.todb(table3, connection, 'foobar13',create=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ya = etl.fromdb(connection, 'SELECT * FROM foobar13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ya.torecarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like hax.minitrees.load, we got a dataframe back. At the moment you can only load one run at a time (but of course this should change).\n",
    "\n",
    "Though it's not related to strax, let's make a quick plot of the events we just loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def event_plot(df):\n",
    "    plt.scatter(df.s1_area, df.s2_area,\n",
    "                c=df.s1_area_fraction_top,\n",
    "                vmin=0, vmax=0.3, \n",
    "                s=10,\n",
    "                cmap=plt.cm.jet,\n",
    "                marker='.', edgecolors='none')\n",
    "    plt.colorbar(label=\"S1 area fraction top\", extend='max')\n",
    "    plt.xlabel('S1 (PE)')\n",
    "    plt.ylabel('S2 (PE)')\n",
    "    plt.xscale('symlog')\n",
    "    plt.yscale('log')\n",
    "    plt.ylim(1e2, 1e7)\n",
    "    \n",
    "event_plot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you figure out what kind of data this is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Waveform analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently strax does not have a dedicated waveform plotter yet. However, a key feature is that the *peaks* array contains the sum waveform information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.data_info('peaks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the compound data type of the `data` field and two others (`area_per_channel` and `width`). While we could load this data with get_df, pandas DataFrames which such fields are very inefficient. A better datastructure is numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks = st.get_array(run_id, 'peaks')\n",
    "type(peaks), peaks.dtype.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot peak waveforms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_peak(p, t0=None, **kwargs):\n",
    "    n = p['length']\n",
    "    if t0 is None:\n",
    "        t0 = p['time']\n",
    "    plt.plot((p['time'] - t0) + np.arange(n) * p['dt'], \n",
    "             p['data'][:n] / p['dt'], \n",
    "             linestyle='steps-mid',\n",
    "             **kwargs)\n",
    "    plt.xlabel(\"Time (ns)\")\n",
    "    plt.ylabel(\"Sum waveform (PE / ns)\")\n",
    "\n",
    "plot_peak(peaks[2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_peaks(main_i, n_before=0, n_after=0, label_threshold=0):\n",
    "    for i in main_i + np.arange(-n_before, n_after + 1):\n",
    "        p = peaks[i]\n",
    "        label = None\n",
    "        if p['area'] > label_threshold:\n",
    "            label = '%.1f PE, %d ns dt' % (p['area'], p['dt'], )\n",
    "        plot_peak(p,\n",
    "                  t0=peaks[main_i]['time'],\n",
    "                  label=label)\n",
    "    plt.ylim(0, None)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.yscale('symlog')\n",
    "\n",
    "plot_peaks(943, n_after=1, n_before=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_peaks(1190, n_after=0, n_before=1, label_threshold=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The abrupt termination of the S2 above is due to strax's data reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following was just as easy as loading data with `hax.minitrees`. However, strax also makes some things that were very difficult with pax/hax easy too. \n",
    "\n",
    "As you can see in the above plot, we have many events high up in the TPC at low S1. Perhaps you might want to get rid of them by increasing the 'S1 coincidence requirement', i.e. the number of PMTs that must see something before a peak is labeled as S1. Then, of course, you want to load the event-level data again to see if it worked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### How it works in pax/hax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    " * Find out which plugin does S1 classification (search on github?)\n",
    " * Look at source code to see what options it takes\n",
    " * Make custom ini or at least `--config_str` option (look up syntax)\n",
    " * Either\n",
    "   * Download raw data from rucio (zzzz...)\n",
    "   * Reprocess data with pax from scratch (look up syntax)\n",
    " * Or: \n",
    "   * Do partial reprocessing, if you can figure out what fields you can load from ROOT file and what plugins are safe to run, then make custom ini that does that, etc...\n",
    " * Point hax to folder with results (using `hax.init`) and figure out how to get hax to ignore the runs db (`pax_version_policy`? `use_run_db`?)\n",
    " * (couple retries because we rely on undocumented file naming conventions to figure out if data exists)\n",
    " * Use hax.load to remake the minitrees you want.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works in strax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to see which configuration option we have to change. Since strax plugins declare explicitly what configuration they take and what other plugins they depend on, this can be done automatically. We just ask which options with `s1` in their name influence `event_basics`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.search_config('event_basics', 's1*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we're after the `s1_min_n_channels option`. Note this is not part of the `event_basics` data type, but of a data type called `peak_classification`. As you can see from the table, this option is not set in the current context, so the default value (3) is used.\n",
    "\n",
    "To try out a different option, just pass it to get_df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = st.get_df(run_id, 'event_basics',\n",
    "                 config=dict(s1_min_n_channels=50))\n",
    "event_plot(df_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice all the small S1 events are indeed gone now.\n",
    "\n",
    "Behind the scenes, this figured out which dataframes had to be remade: as it happens this time just `event_basics` and `peak_basics`. You will now have a new `event_basics_<somehash>` folder in `./custom_data` which contains the results, as well as a new `peak_basics_<somehash> folder`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on configuration changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing configuration can be done in two other ways. We can change it permanently in the current context:\n",
    "\n",
    "    st.set_config(dict(s1_min_channels=50))\n",
    "    \n",
    "Or we could make a new context, with this option set:\n",
    "    \n",
    "    st_2 = st.new_context(config=dict(s1_min_channels=50))\n",
    "    \n",
    "(feeding it to get_df just does the latter behind the scenes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strax also protects you from typos in the configuration. Suppose we typed `s1_min_n_channelz` instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = st.get_df(run_id, 'event_basics',\n",
    "                 config=dict(s1_min_n_channelz=10))\n",
    "event_plot(df_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of get_df is just the same as if the option wasn't set (just like in pax/hax), but you also get a warning about an unknown configuration option. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heavy customization: new plugins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it happens, changing the S1 coincidence requirement is not even configurable in pax, it's hardcoded:\n",
    "  * https://github.com/XENON1T/pax/blob/master/pax/plugins/peak_processing/ClassifyPeaks.py#L29\n",
    "\n",
    "Thus you have to do even more work: either install pax in developer mode, or write your own custom plugin, find out how to get pax to see it, change the config to use it, etc.\n",
    "\n",
    "In hax you would make a custom minitree maker in this case. You can do the same with strax plugin.\n",
    "\n",
    "Suppose you have a brilliant new idea for peak classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class AdvancedExpertClassification(strax.Plugin):\n",
    "    \"\"\"Everything is an S1!\"\"\"\n",
    "    provides = 'peak_classification'\n",
    "    depends_on = ('peak_basics',)\n",
    "    dtype = strax.xenon.plugins.PeakClassification.dtype\n",
    "\n",
    "    def compute(self, peaks):\n",
    "        return dict(type=np.ones(len(peaks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO: explain how to understand plugins like this. Also mention hax-like treemakers, see strax.xenon.plugins.LargestS2Area]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the plugin provides 'peak_classification' and produces the same data type as PeakClassification plugin, which in this case is just:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strax.xenon.plugins.PeakClassification.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use it in place of PeakClassification, we only have to register it. Again, we can do so permanently using `st.register(AdvancedExpertClassification)` or temporarily by feeding it to `get_df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = st.get_df(run_id, 'event_basics',\n",
    "               register=AdvancedExpertClassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_plot(df)\n",
    "plt.yscale('linear')\n",
    "plt.ylim(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, all events are now S1-only events, as expected. Maybe this is not the best alternative classification :-)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
